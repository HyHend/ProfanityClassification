{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profanity Classification\n",
    "Based on [this kaggle challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) called \"Toxic Comment Classification Challenge - Identify and classify toxic online comments\"\n",
    "\n",
    "### Quick research\n",
    "Turned up these papers (pdf warning and such):\n",
    "1. [Text classifcation of short messages. Lundborg et al.](http://lup.lub.lu.se/luur/download?func=downloadFile&recordOId=8928009&fileOId=8928011)\n",
    "2. [Automatic Classification of Abusive Language and Personal Attacks in Various Forms of Online Communication. Bourgonje et al.](https://link.springer.com/content/pdf/10.1007%2F978-3-319-73706-5_15.pdf)\n",
    "3. [Automatic Detection of Cyberbullying in Social Media Text. van Hee et al.](https://arxiv.org/pdf/1801.05617.pdf) and a [different version](https://biblio.ugent.be/publication/6969774/file/6969839.pdf)\n",
    "4. [Harnessing the Power of Text Mining for the Detection of Abusive Content in Social Media. Chen et al.](https://arrow.dit.ie/cgi/viewcontent.cgi?referer=https://scholar.google.nl/scholar?as_ylo=2014&q=profanity+text+classification&hl=en&as_sdt=0,5&httpsredir=1&article=1196&context=scschcomcon)\n",
    "\n",
    "#### A  shallow scan of these papers/thesis:\n",
    "1. They are also looking into a multi class problem. Although generally less than our 7 classes.\n",
    "2. They, among English, are working on Swedish, Dutch (well, Belgium ;)) data.\n",
    "3. Annotation seems to be one of the hardest problems. Already done for us :D\n",
    "4. There could be a nice scientific comparison to peer work. If time\n",
    "5. n-grams (character, word, skip-grams)\n",
    "   - 1-3 word n-grams and 1-6 character n-grams\n",
    "   - skipgrams using NER from spacy, mcparseface or NLTK. For example 2-grams in the list of nouns \n",
    "6. (Feature) Spelchecker results\n",
    "7. (Feature) Sentiment analysis (In this case: #positive words/#total words, #negative words/#total words)\n",
    "8. (Feature) Linguistic features\n",
    "   - #words, #characters\n",
    "   - #uppercase words (normalized)\n",
    "   - #uppercase characters (normalized) (-> avg ratio uppercase characters per word?)\n",
    "   - Longest word\n",
    "   - Average word length\n",
    "   - #one letter tokens, #number of one letter tokens / #words.\n",
    "   - #punctuation, spaces, exclamation marks, question marks, at signs and commas\n",
    "9. (Feature) You can extract sytactic features from word trees (generated by for example spacy, I remember).\n",
    "   - Word + parent\n",
    "   - Word + grandparent\n",
    "   - Word + children\n",
    "   - Word + siblings of parent\n",
    "10. (Feature) Specifically engineered term lists\n",
    "   - Binary features indicating that a term from given list is in the text. Lists are researcher engineered and contain for example words indicating bullying (in the case of paper 3)\n",
    "11. (Classifiers Used) Naive Bayes, SVM, Random Forest, Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "trainPd = pd.read_csv(\"train.csv\")\n",
    "testPd = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, what's in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_text_len_values(column, label):\n",
    "    \"\"\" Given column, calculates numbers such as\n",
    "    mean, median, std dev, min, max text length \n",
    "    of the character counts of the text\n",
    "    and prints these. Also prints a histogram based \n",
    "    on the character counts of the texts\n",
    "    \"\"\"\n",
    "    textLen = column.apply(len)\n",
    "    print(\"{7}:\\n\\tmin:\\t{0}\\n\\tmax:\\t{1}\\n\\tmedian:\\t{2}\\n\\tmean:\\t{3}\\n\\tstddev:\\t{4}\\n\\t10th perc:\\t{5}\\n\\t90th perc:\\t{6}\".format(\n",
    "        np.min(textLen),\n",
    "        np.max(textLen),\n",
    "        np.median(textLen),\n",
    "        round(np.mean(textLen),2),\n",
    "        round(np.std(textLen),2),\n",
    "        round(np.percentile(textLen,10),2),\n",
    "        round(np.percentile(textLen,90),2),\n",
    "        label\n",
    "    ))\n",
    "    \n",
    "    # Show hist\n",
    "    plt.hist(textLen, bins=50)\n",
    "    plt.yscale('log', nonposy='clip')\n",
    "    plt.ylabel('#texts (log)');\n",
    "    plt.xlabel('text length');\n",
    "    plt.show()\n",
    "    \n",
    "def show_data_stats(trainPd, testPd):\n",
    "    \"\"\" Shows/prints information on the dataset\n",
    "    \"\"\"\n",
    "    # Amount of rows?\n",
    "    print(\"Train data:\\n\\t#rows: {0}\\n\\tcolumns: {1}\\n\".format(trainPd.shape[0], list(trainPd.columns)))\n",
    "    print(\"Test data:\\n\\t#rows: {0}\\n\\tcolumns: {1}\\n\".format(testPd.shape[0], list(testPd.columns)))\n",
    "\n",
    "    # Can one line have multiple true labels?\n",
    "    multipleLables = trainPd[np.sum(trainPd[['toxic','severe_toxic','obscene','threat','insult','identity_hate']], axis=1) > 1]\n",
    "    print(\"Can one line have multiple true labels? (train)\")\n",
    "    print(\"\\t#rows with more than one true label: {0}\\n\".format(multipleLables.shape[0]))\n",
    "\n",
    "    # How many of each label is there?\n",
    "    print(\"How many of each label is there?\")\n",
    "    for label in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n",
    "        sumL = np.sum(trainPd[label], axis=0)\n",
    "        print(\"\\t#{0}:\\t{1}\\t(={2}% of train data)\".format(label, sumL, round((sumL/trainPd.shape[0])*100,2)))\n",
    "\n",
    "    # Also print amount of normal samples\n",
    "    sumL = trainPd[np.sum(trainPd[['toxic','severe_toxic','obscene','threat','insult','identity_hate']], axis=1) < 1].shape[0]\n",
    "    print(\"\\t#normal:\\t{0}\\t(={1}% of train data)\\n\".format(sumL, round((sumL/trainPd.shape[0])*100,4)))\n",
    "\n",
    "    # What is the mean, median, std dev, min, max text length? (train and test)\n",
    "    show_text_len_values(trainPd['comment_text'],\"Train\")\n",
    "    show_text_len_values(testPd['comment_text'],\"Test\")\n",
    "\n",
    "    # What is the mean, median, std dev, min, max message length per class? \n",
    "    #  (ignoring the fact that a message can have multiple classes)\n",
    "    for label in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n",
    "        column = trainPd[trainPd[label] == 1]['comment_text']\n",
    "        show_text_len_values(column,\"Train {0} == 1\".format(label))\n",
    "\n",
    "show_data_stats(trainPd, testPd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- Multilabel classification problem. One row can have more than one label\n",
    "- Skewed dataset. Lots (90%) of \"Normal\", only 0.88% of the rows has the label \"threat\" \n",
    "- 90% is of normal class\n",
    "- Lots of rows have more than one label. For example. Of the total, 9.58% in the train data is \"toxic\". Which is almost 100% of the non normal train rows.\n",
    "\n",
    "### What to do?\n",
    "We could (among others):\n",
    "1. Bootstrap our samples in the underrepresented classes\n",
    "2. Undersample the overrepresented classes (Or in combination with 1.)\n",
    "3. Do nothing, let the classification algorithm figure it out (For example Random Forest can handle some skewness)\n",
    "4. Do a binary classification for each of 8 classes (Is it normal or other, is it obscene or other. etc. We can now sample non skewed sets per class)\n",
    "5. Create, from the 7 binary columns one integer (0-127). \n",
    "\n",
    "### What should we do?\n",
    "- I think, 4. Because 1, 2 are hard due to the multilabel properties. Sampling more from the \"threat\" class will also result lots of samples from other classes. We would still have a skewed set. (it is solvable though). Also, this could result in a biased train set.\n",
    "- 3 is also not optimal because it is not slightly skewed. We have a significant skewness. \n",
    "- 4 allows us to create equal sets and still get a probability per class. Which is what the Kaggle challenge asks for.\n",
    "- 5: I for now wouldn't immediately know how to handle the results. We would have to map back from this continuous value to 7 probabilities.\n",
    "\n",
    "_(So, I'm going with 4 and not trying more due to time constraints)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "1. create_binary_training_data\n",
    "   - Creates, for each class a training set with n True and n False samples\n",
    "2. train_on_binary_training_data \n",
    "   - For each class separately calculate features and train an model\n",
    "   - Creates some simple counting features\n",
    "   - Uses create_document_term_matrix to create TF and TF/IDF matrices\n",
    "   - Trains a Random Forest Classifier using Grid Search and 10 fold cross validation\n",
    "3. show_feature_importances\n",
    "   - Shows for each of the models the n most important features\n",
    "   - Allows for visual inspection. Does it make sense? Does it look like we could improve on this? \n",
    "4. validate_on_remaining_train_data\n",
    "   - In train_on_binary_training_data, only part of the training data has been sampled to be used\n",
    "   - Use the rest of this training data as an validation set\n",
    "   - predict on these validation samples using the trained models (in \"predict\")\n",
    "   - Calculate the avg ROC AUC on the results. Kaggle ranks on this value (on the test set that is)\n",
    "5. predict\n",
    "   - Uses the existing models to predict for each class the probability that this comment falls in given class\n",
    "6. save_predictions\n",
    "   - Saves the comment_id's with the predicted probabilities to an CSV (to be uploaded to Kaggle)\n",
    "7. print_random_classifications\n",
    "   - For visual inspection\n",
    "   - Shows comment_text for n random samples with actual label and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_training_data(trainPd, \n",
    "                                classes=['toxic','severe_toxic','obscene','threat','insult','identity_hate'], \n",
    "                                n=500):\n",
    "    \"\"\" Creates binary training data\n",
    "    Returns dictionary with, for each class in classes an dataframe df\n",
    "    where df has two classes of n values. Given class and \"other\" (True = given, False = other)\n",
    "    \"\"\"\n",
    "    trainPds = {}\n",
    "\n",
    "    # Create train datasets for each class. Resulting in two types of rows *class and *other\n",
    "    # Bytheway we're not going to classify anything as \"normal\". It results from \"other\" in all binary classifications\n",
    "    for label in classes:\n",
    "        # Sample \"label\"(if len(label) < n, sample w/o replacement. Otherwise with replacement)\n",
    "        ofLabel = (trainPd[label] == 1)\n",
    "        replace = True\n",
    "        if sum(ofLabel) > n:\n",
    "            replace = False\n",
    "        classPd = trainPd[ofLabel].sample(n, replace=replace)\n",
    "        classPd['label'] = True\n",
    "\n",
    "        # Sample not \"label\" #TODO: Stratified sampling? Or an even amount per class?\n",
    "        # This class will contain LOTS (90%) of \"normal\" values if we don't do anything about it\n",
    "        nonClassPd = trainPd[trainPd[label] != 1].sample(n, replace=False)\n",
    "        nonClassPd['label'] = False\n",
    "\n",
    "        # Add as one (vertically joined) dataframe to trainPds\n",
    "        trainPds[label] = pd.concat([classPd, nonClassPd], axis=0)\n",
    "\n",
    "        # We now have, per label, one trainPd with samples of given label and samples of all other labels\n",
    "        # Note, the True values belong to the label class. But can ALSO belong to other classes.\n",
    "        #       the False values DON'T belong to the label class. But to any or no amount of other classes \n",
    "        #       (With same distribution as the original dataset)\n",
    "    return trainPds\n",
    "\n",
    "binaryTrainingData = create_binary_training_data(trainPd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_simple_features(message):\n",
    "    \"\"\"Cleans message, calculates values on message such as:\n",
    "    - % of capitals\n",
    "    - #characters\n",
    "    - #words\n",
    "    - #punctuation\n",
    "    - #!\n",
    "    - #?\n",
    "    etc...\n",
    "    \n",
    "    Returns pandas series of dictionary with features\n",
    "    \"\"\"\n",
    "    return pd.Series({\n",
    "        'numCapitals':sum(message.count(x) for x in ('Q','W','E','R','T','Y','U','I','O','P','A','S','D','F','G','H','J','K','L','Z','X','C','V','B','N','M')),   # RF model will scale/normalize in respect to num characters\n",
    "        'numCharacters':len(message),\n",
    "        'numWords':len(message.replace('\\n', ' ').replace('  ', ' ').split(' ')),  # Not entirely correct, close enough\n",
    "        'numPunctuation':sum(message.count(x) for x in ('!','@','#','$','%','^','&','*','(',')','.',',','/','\\\\',']','[','{','}','\"',':',';',\"'\",']','`','~','|')),\n",
    "        'numExclamation':message.count('!'),\n",
    "        'numQuestion':message.count('?'),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_term_matrix(messages, \n",
    "                                max_features=1000, \n",
    "                                strip_accents='unicode',  #None\n",
    "                                analyzer='word',\n",
    "                                ngram_range=(1,5),        #Optimize\n",
    "                                stop_words='english',\n",
    "                                lowercase=True,\n",
    "                                max_df=0.9,               #Optimize\n",
    "                                min_df=1,                 #Optimize\n",
    "                                tfidf=False):\n",
    "    \"\"\" Create a document term matrix from given list of messages\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(max_features=max_features,\n",
    "                                strip_accents=strip_accents,\n",
    "                                analyzer=analyzer,\n",
    "                                ngram_range=ngram_range,\n",
    "                                stop_words=stop_words,\n",
    "                                lowercase=lowercase,\n",
    "                                max_df=max_df,\n",
    "                                min_df=min_df,\n",
    "                                )\n",
    "    \n",
    "    if tfidf:\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features,\n",
    "                                strip_accents=strip_accents,\n",
    "                                analyzer=analyzer,\n",
    "                                ngram_range=ngram_range,\n",
    "                                stop_words=stop_words,\n",
    "                                lowercase=lowercase,\n",
    "                                max_df=max_df,\n",
    "                                min_df=min_df)    # Override. Use the tfidf vectorizer\n",
    "    vectorizer.fit(messages)\n",
    "\n",
    "    dtm = vectorizer.transform(messages)\n",
    "    dtmDf = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return dtmDf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(textPd, max_dtm_features=100, vectorizers=None):\n",
    "    \"\"\" Calculates features based on text column of dataframe\n",
    "    Returns features and a dictionary of built vectorizers\n",
    "    \"\"\"\n",
    "    # Simple\n",
    "    print(\"Creating simple features..\")\n",
    "    simpleDf = textPd['comment_text'].apply(calculate_simple_features)\n",
    "    simpleDf = simpleDf.reset_index()\n",
    "    simpleDf = simpleDf.drop(columns=['index'])\n",
    "    \n",
    "    # Based on vectorizers given or not given, calculate or use\n",
    "    if vectorizers == None:\n",
    "        print(\"No vectorizers given. Creating new on data.\")\n",
    "        \n",
    "        # DTM\n",
    "        print(\"Creating document term matrix..\")\n",
    "        print(\"Count..\")\n",
    "        dtmCountDf, dtmCountVectorizer = create_document_term_matrix(list(textPd['comment_text']), max_features=max_dtm_features)\n",
    "\n",
    "        # TF/IDF\n",
    "        print(\"TF/IDF..\")\n",
    "        dtmTfIdfDf, tfIdfVectorizer = create_document_term_matrix(list(textPd['comment_text']), max_features=max_dtm_features, tfidf=True)\n",
    "    else:\n",
    "        print(\"Vectorizers found. Only applying new data on these vectorizers.\")\n",
    "        \n",
    "        # DTM\n",
    "        print(\"Count..\")\n",
    "        dtmCount = vectorizers['dtmCountVectorizer'].transform(list(textPd['comment_text']))\n",
    "        dtmCountDf, dtmCountVectorizer = pd.DataFrame(dtmCount.toarray(), columns=vectorizers['dtmCountVectorizer'].get_feature_names()), vectorizers['dtmCountVectorizer']\n",
    "        \n",
    "        # TF/IDF\n",
    "        print(\"TF/IDF..\")\n",
    "        dtmTfIdf = vectorizers['tfIdfVectorizer'].transform(list(textPd['comment_text']))\n",
    "        dtmTfIdfDf, tfIdfVectorizer = pd.DataFrame(dtmCount.toarray(), columns=vectorizers['tfIdfVectorizer'].get_feature_names()), vectorizers['tfIdfVectorizer']\n",
    "    \n",
    "    # Merge into one feature DF\n",
    "    featureDf = pd.concat([simpleDf, dtmCountDf, dtmTfIdfDf], axis=1)\n",
    "    return featureDf, {'dtmCountVectorizer':dtmCountVectorizer, 'tfIdfVectorizer':tfIdfVectorizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_on_binary_training_data(binaryTrainingData,\n",
    "                                  labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate'],\n",
    "                                  max_dtm_features=1000):\n",
    "    \"\"\" Train an classifier for each of the binary classes in binaryTrainingData\n",
    "    \"\"\"\n",
    "    gscvScores = {}       # To hold results\n",
    "    featuresPds = {}      # To hold dataframes with features\n",
    "    vectorizers = {}      # To hold vectorizers necessary for testing\n",
    "    for label in labels: \n",
    "        print(\"Processing label {0}:\\n------------------\".format(label))\n",
    "        currentPd = binaryTrainingData[label]\n",
    "        textPd = currentPd[['id','comment_text']]\n",
    "        originalLabelsPd = currentPd[labels]\n",
    "        labelsPd = currentPd['label']\n",
    "\n",
    "        # Calculate features\n",
    "        featureDf, vectorizers[label] = calculate_features(textPd, max_dtm_features)\n",
    "        featuresPds[label] = featureDf\n",
    "        print(\"\\n#Features: {0}\".format(featureDf.shape[1]))\n",
    "\n",
    "        # Parameter optimization\n",
    "        parameters = {\n",
    "            'n_estimators': [8, 10, 12], \n",
    "            'max_depth': [None, 5, 10], # commented out because I don't want to wait now. Should be done sooner or later\n",
    "            'max_features': ['auto','log2',0.25,50] #,25\n",
    "        }\n",
    "\n",
    "        # Use random forest\n",
    "        # grid search on given parameters\n",
    "        # Use 10 fold cross validation\n",
    "        print(\"Grid search, cross validation..\")\n",
    "        rf = RandomForestClassifier()\n",
    "        gridSearchCV = GridSearchCV(rf, parameters, cv=10)\n",
    "        cvs = gridSearchCV.fit(featureDf, labelsPd)\n",
    "\n",
    "        # Best parameters and corresponding CV score\n",
    "        # When necessary, more results in: cvs.cv_results_\n",
    "        print(\"\\n--- Results for {0} ---\".format(label))\n",
    "        print(\"Best parameters: {0}\".format(cvs.best_params_))\n",
    "        print(\"With best score: {0}\".format(cvs.best_score_))\n",
    "        print(\"-------------------------\\n\".format(label))\n",
    "\n",
    "        # Append scores to history\n",
    "        gscvScores[label] = {\n",
    "            'score':cvs.best_score_,\n",
    "            'params':cvs.best_params_,\n",
    "            'max_features':max_dtm_features,\n",
    "            'best_estimator':cvs.best_estimator_\n",
    "        }\n",
    "        \n",
    "    return gscvScores, featuresPds, vectorizers\n",
    "\n",
    "startTime = time.time()\n",
    "gscvScores, featuresPds, vectorizers = train_on_binary_training_data(binaryTrainingData)\n",
    "elapsedTime = time.time() - startTime\n",
    "print(\"Training finished in {0}s\".format(elapsedTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_feature_importances(gscvScores, top_n=30):\n",
    "    \"\"\" Look into feature importances. Does it make sense?\n",
    "    Allows for visual inspection\n",
    "    \"\"\"\n",
    "    featureImportances = {}\n",
    "    for label in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']: \n",
    "        fi = gscvScores[label]['best_estimator'].feature_importances_\n",
    "        featureImportances[label] = pd.DataFrame([fi], columns=featuresPds[label].columns).T.sort_values(by=0, ascending=False)\n",
    "        print(\"Feature importances for {0}:\\n\\n{1}\\n\\n\".format(label, featureImportances[label][0:top_n]))\n",
    "show_feature_importances(gscvScores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inPd, labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate']):\n",
    "    \"\"\" Classify on given DF using \n",
    "    the \"best estimators\" for each class\n",
    "    and the precalculated (on train data, without labels) vectorizers\n",
    "    \n",
    "    DF should contain column 'comment_text'\n",
    "    \n",
    "    Returns prediction (continuous, 0-1) for each of the labels to be True\n",
    "    \"\"\"\n",
    "    predicted = {}\n",
    "    for label in labels: \n",
    "        print(\"Predicting on label {0}\".format(label))\n",
    "\n",
    "        # Calculate features on data\n",
    "        # There is double work being done here \n",
    "        #   (for example for each label the simple features are extracted)\n",
    "        #   This should be moved to a different location (or memoized)\n",
    "        print(\"Calculating features..\")\n",
    "        features, vectorizer = calculate_features(inPd, max_dtm_features=500, vectorizers=vectorizers[label])\n",
    "\n",
    "        # Predict on previously trained models\n",
    "        print(\"Predicting..\")\n",
    "        estimator = gscvScores[label]['best_estimator']\n",
    "        predicted[label] = estimator.predict_proba(features)\n",
    "    print(\"Done\")\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def validate_on_remaining_train_data(trainPd, \n",
    "                                     binaryTrainingData,\n",
    "                                     sample=5000,\n",
    "                                     labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate']):\n",
    "    \"\"\" Uses remaining training data \n",
    "    predicts using trained models\n",
    "    calculates and shows average area under curve roc result\n",
    "    \n",
    "    Returns average AUC ROC and for each class the AUC ROC\n",
    "    \"\"\"\n",
    "    # Create a dataframe from the train dataframe \n",
    "    # without all samples on which the models are trained\n",
    "    trainedWith = pd.Series()\n",
    "    for label in labels:\n",
    "        trainedWith = pd.concat([trainedWith, binaryTrainingData[label]['id']], axis=0)    \n",
    "    validatePd = trainPd[~trainPd['id'].isin(trainedWith)]\n",
    "\n",
    "    # Validate on all train samples which have not been used in training\n",
    "    if sample is not None:\n",
    "        samplePd = validatePd.sample(sample)\n",
    "    else:\n",
    "        # Use the entire validation Pd\n",
    "        samplePd = validatePd\n",
    "    predictedValidation = predict(samplePd)\n",
    "\n",
    "    # Calculate average area under roc\n",
    "    resultPd = pd.Series()\n",
    "    for label in labels:\n",
    "        labelResult = pd.DataFrame(predictedValidation[label], columns=['not_{0}'.format(label),label])\n",
    "        resultPd = pd.concat([resultPd, labelResult[label]], axis=1)\n",
    "    roc_auc = roc_auc_score(samplePd[labels], resultPd[labels])\n",
    "    \n",
    "    return roc_auc,samplePd,predictedValidation\n",
    "\n",
    "startTime = time.time()\n",
    "roc_auc, validatePd, predictedValidation = validate_on_remaining_train_data(trainPd, binaryTrainingData)\n",
    "elapsedTime = time.time() - startTime\n",
    "print(\"Avg ROC AUC on our validation data is {0}. Calculated in {1}s\".format(roc_auc, elapsedTime))\n",
    "# Note: This ROC AUC can be based on too little data of some \n",
    "#       classes when a large amount of data has been used for training.\n",
    "#       In anyway, the sampling of train data has impact on the skewness of the validation set\n",
    "#       Take into account when considering this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "predicted = predict(testPd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(inPd, \n",
    "                     predictions,\n",
    "                     csvPath,\n",
    "                     labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate']):\n",
    "    \"\"\" Save results to output csv\n",
    "    \"\"\"\n",
    "    resultPd = testPd['id']\n",
    "    for label in labels:\n",
    "        labelResult = pd.DataFrame(predicted[label], columns=['not_{0}'.format(label),label])\n",
    "        resultPd = pd.concat([resultPd, labelResult[label]], axis=1)\n",
    "    resultPd.to_csv(csvPath, index=False)\n",
    "save_predictions(testPd,predicted,\"test_result_009.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_random_classifications(actual,\n",
    "                                 predicted,\n",
    "                                 labels=['toxic','severe_toxic','obscene','threat','insult','identity_hate'],\n",
    "                                 n=10):\n",
    "    \"\"\"Shows n messages with their classification\n",
    "    \"\"\"\n",
    "    print(predicted.keys())\n",
    "    resultPd = actual\n",
    "    resultPd = resultPd.reset_index()\n",
    "    resultPd.drop(0)\n",
    "    for label in labels:\n",
    "        labelResult = pd.DataFrame(predicted[label], columns=['predicted_not_{0}'.format(label),'predicted_{0}'.format(label)])\n",
    "        resultPd = pd.concat([resultPd, labelResult], axis=1)\n",
    "    resultPd = resultPd.sample(n)\n",
    "    \n",
    "    for index, row in resultPd.iterrows():\n",
    "        print(\"===================\\n\")\n",
    "        print(\"Class:\\t\\tPredicted\\tActual\")\n",
    "        print(\"Toxic:\\t\\t{0}\\t\\t{1}\".format(round(row['predicted_toxic'],2),row['toxic']))\n",
    "        print(\"severe_toxic:\\t{0}\\t\\t{1}\".format(round(row['predicted_severe_toxic'],2),row['severe_toxic']))\n",
    "        print(\"obscene:\\t{0}\\t\\t{1}\".format(round(row['predicted_obscene'],2),row['obscene']))\n",
    "        print(\"threat:\\t\\t{0}\\t\\t{1}\".format(round(row['predicted_threat'],2),row['threat']))\n",
    "        print(\"insult:\\t\\t{0}\\t\\t{1}\".format(round(row['predicted_insult'],2),row['insult']))\n",
    "        print(\"identity_hate:\\t{0}\\t\\t{1}\\n\".format(round(row['identity_hate'],2),row['identity_hate']))\n",
    "        print(row['comment_text'])\n",
    "        print(\"\\n===================\\n\")\n",
    "print_random_classifications(validatePd,predictedValidation,n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle expects result:\n",
    "# id,toxic,severe_toxic,obscene,threat,insult,identity_hate\n",
    "# 00001cee341fdb12,0.5,0.5,0.5,0.5,0.5,0.5\n",
    "# 0000247867823ef7,0.5,0.5,0.5,0.5,0.5,0.5\n",
    "# etc.\n",
    "\n",
    "# Evaluation:\n",
    "# Submissions are now evaluated on the mean column-wise ROC AUC. \n",
    "# In other words, the score is the average of the individual AUCs of each predicted column.\n",
    "# - Which basically is the average of the area under the ROC (True positives vs. False positive curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- Train Cross Validation results are in the 0.8-0.95 range (for all binary classes)\n",
    "- Validation avg ROC AUC is ~0.9 (Note: The validation set has different ratios than the train set because of the sampling (and removel of the train data)\n",
    "- Visual inspection shows some correctness\n",
    "\n",
    "## Conclusion\n",
    "There are still lots of possibilities for improvement. \n",
    "A lot of these have been mentioned in the introduction above.\n",
    "In addition, features from topics resulting from LDA might improve on our results.\n",
    "\n",
    "They are not implemented due to time constraints on my part. \n",
    "Also, for example spell checking and NER tools will take significant processing time. \n",
    "When distributed, this should not be a problem but on my 2 core mac it is ;)\n",
    "\n",
    "In my opinion, the numbers look good. It is fairly accurate (although 10% False classification will result in 15k misclassifications).\n",
    "\n",
    "Visual inspection of the results show some correctness although I find it hard to believe it is near good enough to use. Also it doesn't necessarily seem to be close to numbers resulting from the CV and separate validation. I wouldn't be surprised if something is wronge here somewhere. (And am looking for it ;). It should have to do with the skewness, binary sampling and resulting validation set distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
